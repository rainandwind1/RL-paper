# DDPG Continuous Control With Deep Reinforcement Learning  (2016 Deepmind)

## Abstract 

​	在DQN的基础上将其思想拓展到连续动作领域，提出了一个model-free的基于确定性策略梯度的AC算法，在连续动作领域可以发挥作用，并且在几个baseline实验中取得较好的效果，并且证明了这种算法可以端到端的学习策略，直接从原始的图像像素输入中学习策略。

## Introduction

​	DQN在连续的高维动作空间任务中不能直接使用，受限。由于其目标是找到最大化动作值函数的动作，在连续值函数的情况下需要在每一步中执行可迭代的优化过程。

​	简便的能够使DQN这类方法应用在连续动作领域上的方法是简化离散化动作空间，但是这种方法有很多局限性，表现为维度爆炸，离散后的动作空间维度随着连续动作空间的自由度指数增长，并且这个问题在某些需要细粒度精密控制的情况下更为严重。并且单纯的离散化动作空间会使得某些重要的信息丢失。

​	组合AC方法和DQN方法:

​	Prior to DQN, it was generally believed that learning value functions using large, non-linear function approximators was difficult and unstable. DQN is able to learn value functions using such function approximators in a stable and robust way due to two innovations: 1. the network is trained off-policy with samples from a replay buffer to minimize correlations between samples; 2. the network is trained with a target Q network to give consistent targets during temporal difference backups. In this work we make use of the same ideas, along with batch normalization (Ioffe & Szegedy, 2015), a recent advance in deep learning.

​	用规模大的非线性逼近器学习值函数是困难和不稳定的，DQN却通过两个途径稳定鲁棒的实现了，一、通过异策略的经验回放最小化样本之间的相关性 二、Q-target 来训练网络，提供了一个优化（追踪）目标（DQN把预测Q值当做是监督学习问题，监督学习需要固定的监督信号，如果用同一个网络既做检测又做监督会造成moving target）。

## Background

​	动作是真值，环境完全可观测，强化学习基本概念和初始假设条件。

## Algorithm

​	直接将DQN用在连续动作空间的问题上是不可行的，因为在连续空间中寻找贪婪策略需要在每个时间步优化动作$a_t$，这个优化过程太慢，对于大型的无约束的函数逼近器或者非平凡的动作空间来说这个过程太慢。于是我们在DPG算法的基础上使用AC方法替代它。

​	actor参数的更新：

![image-20200327180610497](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200327180610497.png)

​	为了有效应用硬件加速使用小批量学习成了必要的选择。应用经验回放。

​	与DQN类似使用target net但是没有使用直接的copy权值，而是使用了一种渐进的跟踪权值，渐进的向权值接近，另外，actor 和 critic网络各有一个target指导。两个target同时存在有利于目标函数的稳定收敛。

​	由于输入的不同单位尺度变化不同，造成网络超参数的设置很难达到一个较好的效果。一种处理方法是batch  normalization 将输入的各个维度的数据大小尺度化为较为一致的输入。 这种方法将样本的各个维度输入归一化成具有单位均值和方差的输入。

 ## Results

​	低维状态信息和原始图像像素输入，使用相邻的图像帧来判断速度等参数，

算法流程：

![image-20200327233619482](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200327233619482.png)

## Related work



## Conclusion

​	DDPG 算法寻找解的速度相比DQN在所有的实验上都有显著提高，这也简介告诉我们在有足够模拟的前提下，DDPG可以解决更加复杂的问题。





