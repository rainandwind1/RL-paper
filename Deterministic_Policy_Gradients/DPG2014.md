# Determined Policy Gradient Algorithms 

 2014 DeepMind

## Abstract

​	连续动作下的确定性策略梯度强化学习算法。特别之处在于它是动作值函数的期望梯度。相比于随机策略梯度，确定性策略梯度效率更高，同时为了保证足够的探索，引入了一种异策略的AC算法，从探索行为策略中学习一个确定性的目标策略，在高维空间的优势更加明显。

## Introduction

​	在连续动作空间的问题上PG算法广泛的应用，PG算法通常是对随机策略进行采样并根据累计奖励调整策略的参数。与SGD和GD的关系类似，是否可以仿照他们来使用policy gradient来更新policy参数，以往认为这种确定性的策略梯度并不存在，然而的确存在，并且是按照动作值函数的梯度来更新的，并且确定性策略梯度是随机策略梯度策略方差为零时的极限情况。

​	以一个更实际的视角看，随机PG和确定性的PG有着一个重要的区别：在随机PG中，策略梯度统一考虑了状态和动作空间，而在确定性的策略梯度中，策略梯度只考虑了状态空间，于是随机策略梯度所需的样本将更多，尤其是当动作空间维度比较大时。

​	为了去探索全部的状态和动作空间，通常来说随机策略是必须的，采取异策略的方法，在动作选择时使用随机策略确保能够遍及整个动作空间，但是学习的目标策略是一个确定的策略，使用确定性策略梯度推导出一个异策略的带有可微函数逼近器的AC算法，然后根据函数逼近器的动作值函数梯度更新策略参数，同时引入了确定性策略的相融函数概念，确保策略梯度的估计是无偏的。

​	在几种基线问题上应用确定性策略AC算法，在高维空间任务上确定性策略相比随机策略表现更加优异，同时不需要更多的算力，并且与参数量和维度数目成线性关系。

## Background

### Stochastic Policy Gradient Theorem

​	![image-20200226182142456](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200226182142456.png)

### Stochastic Actor Critic Theorem

​	使用TD的方法用函数逼近器学习出Q(s,a)，因此相对于PG，AC有两个神经网络用来学习产生不同的数值。但是学习出的Q值会产生偏差，但是用估计值与真实值的误差指导Q网络参数的更新则可以达到减少甚至无偏估计的效果。

### Off-Policy Actor Critic

​		![image-20200226220057695](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200226220057695.png)

## Gradients of Deterministic Policies

​	![image-20200226231333826](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200226231333826.png)

### limit of stochastic policy gradient:

​	当随机策略网络的方差等于零时就是确定性策略，这是随机策略的极限情况，于是乎在随机策略梯度下的一系列方法都可以在确定性策略下使用。

## Deterministic Actor Critic Algorithms

### On-policy

​	on-policy的一大问题就是可能会造成对动作或者状态空间的探索不够，但是当环境的噪声足够起到作用时，可以保证足够的探索能力（即使在确定性策略的情况下）。

![image-20200227132255136](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200227132255136.png)

### Off-policy

​	![image-20200227134524663](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200227134524663.png)

## Experients











​	