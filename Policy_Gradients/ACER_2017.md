# Sample Efficient Actor Critic With Experience Replay  2017

## Abstract

创新点：1.带偏差矫正的截断式的重要性采样，2.随机竞争网络架构,3.一种新的置信域优化方法

## Introduction

经验回放通常被用于约减样本的相关性，DQN的两个局限性：

（1）最优策略的局限性限制了它在对抗领域的限制。

（2）在大的动作领域中依据Q函数寻找贪婪策略是非常困难的

PG方法：

A3C方法采样的效率极低？？？

## BackGround and Problem Setup

使用Rt：低偏差，高方差   使用函数近似：低方差，高偏差，于是乎组合Rt和当前的函数逼近值从而在减少偏差的同时，降低方差，这是ACER的一大核心设计原则。

A3C组合利用的方式：![image-20200311225907065](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200311225907065.png)

ACER in discrete action space 尽管使用了A3C的网络，但是也引入了一些新的模块和修改。

## Discrete Actor Critic With Expeience Replay

### Multi-Step Estimation on The State-Action Value Function

### Continuous ACER

SDN（随机竞争网络）

![image-20200316121324933](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200316121324933.png)

TRP（置信域更新）



## Concluding Remarks













