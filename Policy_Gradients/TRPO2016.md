# Trust Region Policy Optimization(2016 置信域策略优化)

## 摘要

​	找到一种保证单调递增的优化策略的迭代程序（流程），TRPO类似PG，但能够大型的非线性策略（例如神经网络），实验证明了该算法在很多任务上的鲁棒性，例如仿真机器人的游泳、单足跳、步态学习；雅达利游戏（图像输入），尽管它的估计和理论是有些偏差的，但是TRPO倾向于提供了一个单调的性能提升，并且调节的超参数的工作量很小。

## 介绍

​	大部分的策略优化的算法可以被广义的分为以下三个大类：（1）策略迭代技巧，在提升当前策略下的值函数或者直接提升策略之间做选择；（2）策略梯度方法，直接估计样本轨迹的奖励的梯度，与策略迭代有着紧密的关系；（3）无梯度优化方法，例如交叉熵方法CEM（cross-entropy method），或者协方差矩阵调节CAM（把奖励reward当做一个策略参数的黑盒函数优化）

​	CEM和CAM以其易于理解和实现的优点在很多问题中受欢迎，这篇文章首先证明了最小化某个目标函数可以保证策略的提升，然后为这个微调的算法做了一系列的估计，最终产生了TRPO。这个算法有两个变体：（1）single-path 单步方法，可以用在modell-free中；（2）vine 爬藤算法，要求系统能够存储特定的状态，只能用在仿真中。这些算法具有可伸缩性，实验中证明了TRPO方法可以直接从原始图像中学习复杂的策略。

## 背景（前言）

​	优势值函数$A_{\pi}$的定义：![image-20200105164704218](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200105164704218.png)

## 全局随机策略的单调递增保证

![image-20200306210742085](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200306210742085.png)

单调递增策略迭代算法

解带约束的优化问题，得到更新的策略，直至算法收敛。

![image-20200309111037208](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200309111037208.png)

## Optimization of Parameterized Policies

使用惩罚因子的一个问题就是步长不能大。只能很小，为了避免这个弊端的做法就是将惩罚因子变成一个置信域的约束限制新策略和旧策略的KL散度。

![image-20200309121847525](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200309121847525.png)![image-20200309122742572](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200309122742572.png)



但是（11）的条件比较严格，这里弱化近似了一下改成（12），实验证明了这种弱化修改后的性能与最大（11）类似。

## Sample-Based Estimation of the Objective and Constraint

经过变换，优化问题等效成如下：

![image-20200309150941805](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200309150941805.png)

### Single Path

used for PG，based on sampling individual trajectories

### Vine(lower variance 性能更好，但是效用次数更多，需要系统具备任意状态重置的能力)

构造Rollout集合，在Rollout集合中的每个状态中执行多个动作。Policy iteration

## Practical Algorithm

算法步骤：（包括single path 和 vine）

1.使用MC方法估计Q-values 在single path 或者 vine算法采集的状态对中。

2.通过平均样本值，重建（14）中的目标数

3.通过解出约束优化问题的近似解更新策略参数$\theta$ ，使用共轭梯度加线性搜索算法。细节见附录c 

## Experiments

ALE 平台的 雅达利游戏和运动关节控制上的实验证明了TRPO算法的通用性。













​	

​	





