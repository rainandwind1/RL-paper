# Scalable trust-region method for deep reinforcement learning  using Kronecker-factored approximation(2017)

## 摘要

​	利用克罗内克因子近似曲率在深度强化学习中使用置信域的方法，在自然策略梯度的基础上使用该方法， 取名为Actor Critic Kronecker-Factored Trust Region(ACKTR)，这是第一个可伸缩的置信域自然策略梯度用于ac的方法，是一种从原始图像输入中学习连续和离散控制任务的具有重要意义的算法，我们在离散控制领域使用了雅达利游戏策略测试，在连续控制上使用了 MuJoCo 环境，与先前的最先进的同策略相比，我们达到了更高的奖励水平，并且样本的利用效率提高了好几倍。

## 介绍

​	深度强化学习在连续控制领域等都取得了非常好的成绩和进展，但是当前的深度强化学习神经网络的训练大多基于简单的SGD算法或其变种，SGD以及相关的一阶方法对于权重的开发实在是效率太低了，通常一个算法的训练耗时需要几天才能在连续控制或者离散控制任务中取得不错的控制效果，分布式的方法如（A3C）等确实可以提高训练的速度，但是采样效率的回报会随着并行度的增加而减少。

​	在RL领域里样本利用效率是一个备受关注的领域，有效减少样本的数量尺度的方法之一就是使用新的梯度优化方法，自然策略梯度使用自然梯度下降的方法进行梯度更新，自然梯度是流形上的最陡下降方向，但是自然梯度的计算是非常棘手的由于有求逆的操作，TRPO在大模型下是不合实际的，由于其需要大量的样本并且需要多步求解过程。kronecker-factored是一种可伸缩的自然梯度逼近方法。在监督学习中被用于加速神经网络的训练，使用批处理的方法，不像TRPO，与TRPO不同的是，每次更新的成本与SGD更新相当，并且它保存了曲率信息的运行平均值，允**许它使用小批量。**这说明将K-FAC应用于政策优化可以提高现有deep RL方法的样本效率。也是第一个使用高斯牛顿法优化自然策略梯度，事实上ACKTR的每次更新的计算量仅仅高于SGD 10-25%，并且ACKTR可以持续提高样本的效率以及代理表现与a2c和TRPO想比较。

​	CEM和CAM以其易于理解和实现的优点在很多问题中受欢迎，这篇文章首先证明了最小化某个目标函数可以保证策略的提升，然后为这个微调的算法做了一系列的估计，最终产生了TRPO。这个算法有两个变体：（1）single-path 单步方法，可以用在modell-free中；（2）vine 爬藤算法，要求系统能够存储特定的状态，只能用在仿真中。这些算法具有可伸缩性，实验中证明了TRPO方法可以直接从原始图像中学习复杂的策略。

## 背景（前言）

## Natural gradient using Kronecker-factored approximation

一般的欧几里得范数下的梯度下降法欧几里得范数的改变取决于参数$\theta$，而模型的初始化是任意的这使得优化轨迹受到影响，

分布式的克罗尼克因子优化方法可以加速监督学习中网络的更新优化速度。分布式K-FAC在训练大型现代分类卷积网络时，速度提高了2- 3倍。

## Methods

### Natural gradient in actor-critic

### Step-size Selection and trust-region optimization

SGD的更新规则会导致策略的大规模更新，导致算法过早地收敛到一个近乎确定性的策略，

## Related work



## Practical Algorithm

## Experiments

a2c会随着batch size的增大性能变差, ACKTR不会

## Conclusion

K-FAC进行自然梯度的更新，trust region进行稳定性优化，第一个 actor和critic同时进行自然梯度优化的工作，相比于一阶的方法（A2C）与二阶的方法（TRPO）我们的方法在样本效率上提高了2-3倍，由于算法的可扩展性，我们也是第一个直接从原始像素观测空间训练连续控制中几个重要任务的人。













​	

​	





