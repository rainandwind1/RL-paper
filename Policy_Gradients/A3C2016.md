# Asynchoronous Methods for Deep Reinforcement Learning (A3C) 2016 Gol DP

## Abstract

  异步梯度下降优化控制器 实现了四种基本强化学习算法的异步版本，表现最优的是Actor critic

## Introduction

   Simple online RL algorithms + Deep neural networks -> fundamentally unstable  but a variety of solutions have been proposed to stabilize the algorithms.

​	这些解决方法有一个共同的核心：围绕着两个问题：（1）online RL agent 处理的观测序列数据是非平稳的，（2）并且RL的更新之间是高度相关的  

​	于是产生了通过将代理数据存储在回放的经验中或者在不同时间步中进行随机采样的方法  通过收集经验的方式有效减少数据的非平稳性和更新间的相关性，但是同时也限制了**异策略方法**的使用。

​	基于经验回放的deep RL 算法的缺点：存储空间要求大，对于异策略的要求较大，需要能够从老策略采样数据更新新的策略。

​	Instead of experience replay, asynchronously execute multiple agents in parallel , on multiple instances of environment. 在多个环境实例中异步并行的执行多个代理，这种方法可以有效的降低代理数据之间的关联性并且可以提高数据的平稳性。这种方法可以使得多数使用深度神经网络的同策略和异策略的强化学习方法更加鲁棒。

​	这种异步框架的另一个实际优点： 很多强化学习算法对于硬件（GPUs）或者分布式的架构依赖较强，而这个框架可以运行在一个拥有多核CPU的单个机器上。并且能够达到一个非常好的效果，在时间上也有明显的优势，相对于那些分布式的方法占用更少的资源。

​	The best of proposed methods, asynchronous advantage actor-critic (A3C) ,我们认为，A3C在2D和3D游戏、离散和连续动作空间上的成功，以及其训练前馈和递归代理的能力，使其成为迄今为止最普遍、最成功的强化学习代理。

## Related Work

  并行的进化的方法被提出并且应用在了一些视觉的强化学习任务中。

## Reinforcement Learning Background

​	one-step Q learning 学习速度慢，不仅如此，one-step限制了奖励信息的传播，使得奖励的影响范围局限在一步动作中不能传递到相关的状态和动作中去。n-step方法缓解了这种影响。

​	与值函数方法的相反的是基于策略梯度的方法，直接使用梯度上升方法。

## Asynchronous RL Framework

​	multi-threaded asynchronous variants of one-step Sarsa,one/n -step Q-learning and advatange actor critic

​	use multiple CPU threads on a single machine 节省了机器间交流传递梯度和参数信息的时间和消耗。让多个学习器并行的观测环境从而能够探索到环境中更多不同的状态，并且不同的学习器可以使用不同的探索策略，使得多样性得到最大化，单个学习器间参数更新的相关性由于并行在线更新的缘故相比单个代理来说得到大大的降低，从而可以不再使用经验回放而依赖并行学习器来替代DQN中经验回放所扮演的角色。

​	同时为了稳定学习的性能，多并行actor-learners的使用有很多优点：首先是在训练时间上与学习器的个数成线性关系；第二，使得on-policy Sarsa等算法可以稳定的使用。

### 1. Asynchronous one-step Q-learning

​	累积梯度更新(minibatches)可以减少学习器间互相覆盖更新的几率 。给予不同学习器不同的探索策略可以有效的提高算法的鲁棒性。

![image-20200222162701042](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200222162701042.png)



### 2. Asynchronous one-step Sarsa

### 3. Asynchronous n-step Q-learning

### 4. Asynchronous advantage actor critic

​	

总结：异步并行的学习器+累积梯度 ->提升了稳定性，降低了相关性

### 5. Optimization

(1) SGD with momentum   (2) RMSProp 

## Experiments

​	这种并行机制的优势在于再相同的时间内可以吞吐更多的数据同时可以采取不同的探索策略，但是达到某个分数所需要的步数是相同的。同时使得模型的效率得到线性的提高。使得在同样的情况下，由于数据使用的效率更高，越多的学习器并行运行，所需要的数据越少。

![image-20200223174751452](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200223174751452.png)





​	

​	



​	

​	

