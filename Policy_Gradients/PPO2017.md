# Proximal Policy Optimization 2017 OpenAI 

## Abstract

​	提出了一类新的策略优化的算法，通过采样代理与环境交互的数据，使用随机梯度上升的方法优化替代目标函数，标准的策略梯度方法在每个数据样本上进行一次梯度的更新，我们提出了一种能够在多个回合内进行小批量的更新，新方法（Proximal Policy Optimization）拥有TRPO（Trust Region Policy Optimization）的一些优点，但是在实现上更简便，并且通常更易采样，并且与其他方法相比，PPO在样本复杂性、简洁性和实时性上能够取得较好的平衡性。

## Introduction

​	近些年的主导研究Deep Q-learning (在一些简单问题上失败，解释性差)/vanilla PG(数据效率低，鲁棒性差)/TRPO(较为复杂，与那些包括drpoout、参数共享的架构不兼容) 等。

​	这篇文章企图使用一阶优化保持TRPO的数据效率和可靠的表现性能。创新性的提出剪切比率，估计策略表现的下界，为了优化策略，我们在策略的抽样数据和对抽样数据执行几个阶段的优化之间交替进行。

## Background

### 1. Policy Gradient Methods

### 2. Trust Region Methods (给定KL散度约束下最大化)

### 3. Clipped Surrogate Objective(剪切比率)

### 4. Adaptive KL Penalty Coefficient(自适应的KL惩罚系数)

​	为惩罚系数设置一个目标值，每次迭代计算KL散度的数值，不同的KL散度对应的惩罚系数更新（增大or减小）方式不同。

## Algorithm

![image-20200225102858154](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200225102858154.png)

## Experiments

## Comparison to other Algorithms in the continuous domain







​	

​	