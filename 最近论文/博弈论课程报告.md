# Mastering Complex Control in MOBA Games with Deep Reinforcement Learning(AAAI 2020)

姓名：张朋朋   学院：人工智能学院   学号：2019E8014661010

​		今天我将要分享的论文是腾讯AI lab 2020年发表在AAAI会议上的一片论文，论文的题目是《Mastering Complex Control in MOBA Games with Deep Reinforcement Learning》，其主题为使用强化学习训练出能在腾讯MOBA游戏王者荣耀中超越人类水平的AI。当然先说这里他们的效果非常好，已经基本实现了远超人类水平的目标，让我们共同对其模型架构以及他们解决各种问题的方法进行讨论和概述。

## 王者荣耀游戏环境介绍

​		王者荣耀是腾讯旗下的风靡全球的MOBA类手游，其基本游戏模式为5v5的团队竞技游戏模式，目标是首先吧对方的水晶（home）击毁，其中玩家操控的英雄有上百种，并且每个英雄的技能各异，并且技能间的组合应用及其考验技巧，但是本篇论文主要基于的是该游戏的1v1模式，其地图设置与5v5稍有不同，获胜机制是一样的，仅仅是游戏玩家的个数上的差异，其中环境中的图像输入信息在本篇文章的后续部分会详细介绍。

## 背景以及基本工作介绍

​		首先这篇论文说明了王者荣耀这类多人在线对抗游戏中的1v1比赛类似传统的1v1比赛如（围棋，国际象棋等等），但是又显然在机制复杂度等方面不同于强化学习在先前的能够胜任的1v1游戏环境。并且在论文中列出了MOBA 1v1与Go 1v1相比双方在动作状态空间，训练数据以及游戏机制上的差别，这里如下图显示：

![image-20200607213003086](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200607213003086.png)

​		由上图我们可以明显的看出来MOBA 1v1相比较Go 1v1上在动作和状态空间上都明显更大，并且在可以使用的人类训练数据上MOBA 1v1是明显少于GO 1v1的，在游戏机制上MOBA 1v1偏向于实时的复杂控制，而GO 1v1则是偏向于长期的策略打法。在MOBA 1v1游戏中要实时的搜索到较好的策略显然更为困难，这篇文章针对此类问题提出了一个新的深度强化学习架构以及深度强化学习算法，这个新的架构具有低耦合和高度的可伸缩性，后者可以使该模型方便的配置到不同尺度下的训练环境，而新的算法包括了控制依赖解构、动作蒙版和目标注意力机制等技巧，并且在Clip PPO 的基础上创新性的提出了Dual-clip PPO算法以适应大规模批处理中样本异策略的问题，最终的模型和算法效果也是很不错，下面我将会分别对模型和算法上的关键创新点进行概述。

## System Design

​		复杂的代理控制会引出随机梯度的高方差，但是对于算法来说大规模的批处理又是加速训练的关键因素，基于此目的设计了一个可伸缩的松耦合系统来构建数据并行的使用。系统的组成如下：

![image-20200607221902647](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200607221902647.png)

（1）AI Server: 负责环境与 AI 的交互, 通过 self-play 与镜像策略生成回合数据，使用玻尔兹曼探索产生对手策略，softmax 输出。动作执行后回收奖励等信息，AI服务器与推理模型在同一个CPU为了节省IO输出，快速推理版本为feather CNN（生成episode快速），这个快速版本可以自动的将算法模型转化成用于推理的格式。

(2) Dispatch：这个组件的功能主要是将AI Server 与环境交互后收到的奖励等环境信息压缩，加压之后发送到 Memory Pool 中。

（3）Memory Pool：负责存储样本数据，按照 RL learner 所需的输入标准处理存储数据，并负责给 RL Learner 提供 data batch，这里需要说明的是这个模块提供不同长度的样本批次，以及不同生成时间的数据。

（4）RL Learner： 分布式训练环境，为了加速策略更新使用batch size，多学习器混合并行从memory 池中取数据，用ring allreduce 算法平均梯度，并行训练，但不同与IMPALA，在IMPALA中，参数分布在各个学习者之间，参与者并行地从所有学习者处检索参数。

## Algorithm Design

​		为了高效的训练这个网络，几种创新的策略被用在了上面，第一个是目标注意力机制：帮助选择组合单位的目标，第二个是LSTMs：帮助学习组合技能以及时给予对方以伤害和持续打击，第三个是控制依赖解耦以形成多标签近端策略优化(PPO)目标。第四个是基于游戏知识的剪枝方法，action mask，最后是双端剪切的PPO算法，保证算法在大批次的收敛性。

![image-20200607223302006](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200607223302006.png)

（1）目标注意力机制：生成对目标单元的个数以及目标单位的预测，以有效的获取潜在对手的信息，及时作出反应。

（2）LSTMs：可以有效的将各个动作以及技能之间的关系学习出来，以更好的增加AI对技能与走位的认识。

（3）控制依赖解构：将动作输出的每一维度进行PPO的剪切目标的求取，这样可以间接的学习出动作之间的关联性，使得动作的组合更加的有目的性。

（4）基于游戏基本知识或规则的剪枝方法，action mask：就是按照游戏规则对一些冗余的不符合规则和常理的动作状态剪去，可以大大减少更新量，提升算法的更新收敛速度。

（5）双端剪切PPO算法：传统的Cilp PPO 算法的损失函数为

​	$\mathcal{L}^{CLIP}(\theta) = \hat{E}_{t}[min(r_{t}(\theta)\hat{A}_{t},clip(r_{t}(\theta), 1 - \epsilon,1+\epsilon)\hat{A}_{t})]$

而PPO是基于on-policy的算法，此论文中由于对memory pool中采样更新的操作的样本数据涉及到大量混合的不同策略，这里再使用传统的剪切版本的PPO将会导致重要性采样比率$r_{t}(\theta)$过大，导致算法不稳定，因此在此基础上他们提出了双端剪切的PPO算法，原算法在优势值函数为负时，取

$\mathcal{L}^{CLIP}(\theta) = max(r_{t}(\theta),clip(r_{t}(\theta), 1 - \epsilon,1+\epsilon))\hat{A}_{t}$

此时可能造成损失函数过大，从而造成不稳定，于是我们对优势值为负值时再做处理如下：

$\mathcal{L}^{CLIP}(\theta) = \hat{E}_{t}[max(min(r_{t}(\theta)\hat{A}_{t},clip(r_{t}(\theta), 1 - \epsilon,1+\epsilon)\hat{A}_{t}),c\hat{A}_{t})]$

此时将会把优势值为负值时的损失函数的绝对值大小限制住，如下图所示：

![image-20200608092550179](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200608092550179.png)

其中(a)为原始的Clip PPO在优势值为负值时的情况，(b)为Dual-cilp PPO在优势值为负值的约束情况，经过此种修改后的PPO算法可以适应到他们提出的大规模批次处理以及随机采样样本的架构中，并且效果不错。

## Experiments

![image-20200607223719646](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200607223719646.png)

​		上图是算法训练后与不同顶尖玩家1v1的胜率，我们可以明显的看到在这8个英雄2100场的战绩里，RL AI的胜率达到了恐怖的99.81%，可以基本上说是稳操胜券，训练后的效果非常不错。

![image-20200607223744382](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200607223744382.png)

​		上图是算法的烧蚀实验，针对动作蒙版剪枝（MA）、注意力机制（TA）以及LSTM的作用分别做了烧蚀实验，实验证明动作蒙版剪枝可以有效的提升算法的收敛速度，但是其对胜率的作用并不大，注意力机制和LSTM对于提升胜率都起到了非常重要的作用。

![image-20200607223758535](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200607223758535.png)

​		上图是深度强化学习AI的天梯分数（排位）水平随着训练时间的改变和提升，这里训练时间在8小时左右RL AI可以完全获胜于行为树搜索的AI，在10小时左右可以达到前10%的顶尖玩家水平，30小时左右可以达到前1%顶尖玩家的水平，60小时左右可以达到前0.01%的顶尖玩家水平，训练70小时左右可以达到职业玩家的水平，之后是超越职业玩家的水平。

## Conclusion & My opinion

​		在本文中，作者提出了一种深度强化学习(DRL)方法来处理MOBA 1v1游戏中代理的复杂动作控制，从系统和算法两个角度都提出了较为新颖和有效的方法和结构，值得称赞和学习，该团队下一步也将开放框架和算法，并将《王者荣耀》游戏核心开放给社区，以促进对复杂游戏的进一步研究;我们还将通过虚拟云为公众提供部分计算资源，这里我觉得计算资源其实也是一个非常重要的点，能够应用和拓展此框架必须有计算资源的加持，此外他们提出的双端剪切PPO算法其实本身并不是很大的改变，相当于一个小的tip，但是其能适合本身的框架就是一种成功，动作目标解耦也是相当于针对此架构的一种tip，并且此操作在动作维度大的时候会造成更新的缓慢以及误差的增大，总的来说改论文能够在MOBA 1v1的场景中首次提出一个不错的系统架构以及其适合的算法，其工作是得以肯定的。