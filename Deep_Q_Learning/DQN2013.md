# Playing Atari with Deep Reinforcement Learning (深度强化学习玩雅达利游戏 2013年 AlphaGo2016年）

## 一、摘要

​	第一个成功应用深度强化学习学习控制策略的模型  卷积神经网络  Q-learning  输入是原始像素，输出是值函数   在7款雅达利2600上做了测试，其中6款表现不错，3款性能达到专家级别。

## 二、简介

​	直接从高维感知数据如图像语音输入中学习控制策略是一项挑战，之前的强化学习非常依赖人为的特征表示。强化学习在线性值函数和手工特征表示领域上应用成功的实例较多。

​	计算机视觉和语音识别上的突破    CNN MP RBM RNN 这些技术是否也可以用在RL学习中，对于高维原始感知数据的处理上。

​	强化学习的挑战：1）奖励信号的稀疏、噪声和延迟。2）动作和结果之间的延迟，可能很长。3）大多数的深度学习的算法基于样本数据是独立的基础之上，然而强化学习的状态序列通常是高度相关的。

​	这篇论文证明了CNN可以克服这些挑战并且能够在复杂的强化学习环境中成功的从原始的视屏输入中学习策略。基本方法是Q-learning算法，SGD更新权值，这里要解决样本数据之间的相关性和非平稳分布的问题，使用了**经验回放**的机制，即随机的从之前的状态转换中采样，从而平滑了对过去许多行为的训练分布。

​	应用在了ALE环境中的雅达利2600游戏中。

## 三、背景

​	考虑动作状态序列，目标函数：

![image-20200103152036700](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200103152036700.png)

其中：![image-20200103152145160](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200103152145160.png)

这里与监督学习不同的是target（yi）也是由一个神经网络学习得到，而不是监督学习中固定的target。

权值的梯度：

![image-20200103152739650](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200103152739650.png)

每步更新

:yellow_heart: On-Policy:同策略，行动策略和目标策略是一致的，也就是说更新target时选动作的策略和行动策略是同一个策略（e-greedy），在更新完target后直接使用选择的动作即为下一步行动所采取的动作。

:red_circle: Off-Policy:异策略，行动策略和目标策略不一致，更新target时选择动作是基于贪婪策略，而行动策略是e-greedy策略。

## 四、相关工作

​	TD-gamma在西洋棋程序中的应用  RL+self-play model free，用了一个隐层的多层感知器来估计值函数，但是在围棋等领域确不尽人意。

​	与此同时由于把model-free的RL与非线性函数逼近器或者与异策略学习相结合会使得Q-network发散，所以当时很多人的注意力集中在了设计保证收敛性的线性函数逼近器上。Deep NN用在了评估环境，RBM用在了值函数的评估或者策略的评估，当评估一个固定的target policy时使用非线函数逼近器收敛性的证明（TD方法），但是这些方法都没有延伸到非线性的控制。

​	前人所做的最相近的工作NFQ（Neural fitted Q-learning），但是本论文实现了端到端的学习

## 五、深度强化学习（Deep Reinforcement Learning）

### 5.1经验回放

​	存储代理每一步的的历史记录et=(St,at,r,St+1,done_flag)，形成记忆库Memory D，从记忆库中采样多组回合数据形成一组回放记忆，内部循环使用Q-learning方法更新target，（mini-batch sampled from D）算法伪代码如下：

![image-20200103164151343](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200103164151343.png)

算法相比经典的Q-learning算法的优势：1）每个经历都有可能对多个权值的更新进行贡献，提高了数据的使用率。2）随机抽取mini-batch打破了样本之间的相关性，减少了更新的方差。3）同策略的学习会影响到next step的动作选择，很容易陷入局部极小和发散的情况。4）通过经验回放技巧平滑了学习过程，避免的参数的震荡和发散。但是均匀采样的方式会使得数据本身的重要性没有被使用，更为精细的采样方式有优先回放（prioritized sweeping）

### 5.2预处理和模型结构

​	raw data:210 x160的图像帧 128的颜色板计算量 这里做数据的预处理  首先是RGB转成灰度数据，接着下采样成110x84的图像，然后裁剪成84x84的主要图框，使用GPU实现二维的卷积运算，（此GPU需要等长的二维数据）

​	所以输入是84x84x4(4帧图像合为一组)的数据，第一层隐层CNN 16个8x8个滤波器 stride = 4，应用一个非线性整流器；第二层用32个4x4的滤波器，stride = 2，带一个非线性的整流器；接着是一个全连接层由256个整流单元组成的隐层单元，输出层是一个全连接的线性层，每个动作一个输出（Na个）， 将这个卷积神经网络称为DQN。

## 六、实验测试

​	对七个雅达利小游戏做了测试，他们中网络结构都相同，只是在训练时的奖励机制不同，由于不同的游戏的节奏等原因。奖励限制在（-1,0,-1）中，便于统一参数设置和量级。epsilon从1->0.1，代理每k帧选择一次动作，不是每帧选择一次动作，这次动作重复k帧。

### 6.1训练和稳定性

​	尽管经验回放没有理论上的收敛保证，但是实验结果和曲线却说明了Q值没有发散，且比较稳定。

### 6.2值函数的可视化

​	有一组帧和值函数的变化曲线配合相应的视频帧说明学习出的值函数对复杂序列的解释性和包含性。

### 6.3主要评估

​	在没有先验知识（输入时原生的灰度数据）的情况下，相比其他的学习方法，DQN有明显的优势。

## 七、结论

在没有调节模型结构和超参数的情况下在几款游戏上取得了较好的结果。

​	





​	

