# Prioritized Experience Replay

## Abstract

​	经验回放使得在线的RL代理能够记忆并对过去的经验进行再利用，而之前（DQN）的经验回放仅仅是以最初经历过的状态等频率的回放，并没有考虑它们的实际意义，这篇文章提出了一个优先级的经验架构，从而使得那些重要的经验得到反复利用，并在DQN中使用，得出的效果比之前的DQN好。

## Introduction

​	传统的强化学习算法在单次更新参数后立马舍弃了输入数据，从而出现两个问题：1.强烈相关的更新打破了许多随机梯度下降算法的假设 2.迅速的忘记了可能有用的稀有经验；而经验回放则是应对了这两个问题：首先在更新中经验回放通过混合或者减少最近的经历来打破暂时的联系，并且稀有的经验也可以重复的被利用，这是稳定DQN中使用神经网络的函数逼近的关键，并且经验回放的方法可以使得数据得到充分的利用，相比于与环境进行交互，经验回放效率更高。（经验回放可以减少学习所需的经验量，并使用更多的计算和更多的内存来代替它——与RL代理与其环境的交互相比，这些资源通常更划算。）

​	在这篇文章中，阐述了带有优先级的状态转换相比等级一致的可以使得经验回放的效率大大提高，也就是说这些状态转换历史中有的是多余的，无用的，有的是与任务密切相关的，有的则是暂时没有起到作用，但是可能在以后作用增加的，相当于让代理拥有了选择回放哪些有用经验的能力。

​	使用TD误差来衡量具有高期望学习过程的经验回放的状态转换，当然这种优先级会导致多样性的缺失，可以通过随机优先的来缓和，同时使用重要性采样的方式减少引入的偏差。实验证明效果很好。（雅达利游戏）

## Background

​	神经科学 海马体  带有reward的经验回放频率更高，  使用随机的基于TD误差的优先回放方法。TD误差还可以作为确定或者选择探索和特征的机制，在监督学习中，

## Prioritized Replay

优先级评价的标准是优先回放的核心组成部分。

### Prioritizing with  TD-Error

TD-error可以间接的体现当前转换的好与坏，确切的说是当前状态距离下一个估计值有多远，非常适合增量式的在线学习强化学习方法，例如Sarsa,Q-learning这类使用TD-error进行参数更新的算法，但是在一些奖励有误差的环境中，TD-error也会起到不好的效果。

### Stochastic Prioritization

最高TD-error的转换会被频繁的使用，使得系统的多样性减少并且容易过拟合。于是我们采用一种在贪婪优先和一致随机优先之间的方式，既保证单调性又保证非零性。

![image-20200305092402546](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200305092402546.png)

其中$P_i$变量有两种定义方式如下：

![image-20200305093506767](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200305093506767.png)

实现起来需要特殊的方法，第一种需要分段采样，第二种需要和数的结构，具体细节见论文。

### 对偏差退火处理？*对偏差进行处理*

​	优先回放引入了偏差因为它以一种不受控制的方式改变了分布，因此所得到的解也会收敛到不同的点，可以通过重要性采样的方式来矫正这种偏差，这是加权重要性采样，而非普通的重要性采样，并通过归一化正则。从而不会增大数值。

​	重要性采样与优先回放结合有另一个优点

实验中基于Double Q-learning 并将其中普通的经验回放换成优先回放加重要性采样的操作

**算法流程：（Double DQN with proporitional prioritization）**

![image-20200305093623244](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200305093623244.png)

## Atrai Experiments

## Discussion

comparison between rank-based and proporitional prioritization

控制回放哪些记忆，同时可以控制选择哪些记忆存储以及记忆库中的哪些记忆可以删除（冗余），将回放的精力更多的投入到哪些误差较高的地方去。

## Conclusion







​	

​	

​	

​	