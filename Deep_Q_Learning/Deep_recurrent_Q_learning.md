# Deep Recurrent Q-Learning for Partially Observable MDPs 2017

## Abstract

​		通过将DQN中的卷积层换成LSTM循环神经网络，在闪烁屏的雅达利游戏上表现出DQN的水平。--DRQN

​		在部分可观测的环境训练网络，递增的用基于更完整的观测环境评估网络时，发现DRQN的性能随着可观测度而变化，当反过来用全观测的环境训练DRQN，用部分观测的数据评估时，DRQN的性能下降的比DQN少，因此，RNN能够在历史中堆积图像帧，这种循环神经网络的性能并没有在玩游戏时表现出系统的优势，但是当环境观测值的质量发生变化时，DRQN能够表现的更好些。

## Introduction

​	DQN在雅达利游戏中将游戏图像输入的四帧作为一个单位特征输入，这使得DQN在原则上并不能记住这四帧之外的历史数据，于是使得游戏变成了部分可观测的MDP问题，证明了DRQN可以有效应对这种部分可观测带来的挑战，并且性能优于DQN。

​	讲述DQN的原理。

## Partial Observability

​		真实世界中满足马尔科夫性的要求很少，部分可观测带来的问题，DQN中学出来的Q值优于部分可观测的原因，所对应的状态与实际的状态相差较大，因此泛化的效果并不理想，当观测不能反映环境的底层信息时。

![image-20200501183445488](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200501183445488.png)

由于循环神经网络的作用，DRQN能够更好的估计系统底层的状态，其根本原因是循环神经网络的加入使得部分观测的状态与实际状态更为贴近，或者说更好的表述了真实状态，从而你能够使得其逼近Q函数的能力和学习策略的能力在部分可观测的设置下表现优于传统算法。

## DRQN Architecture

### Stable Recurrent Updates 

架构的更新：由于循环和卷积的并存，更新网络的方式有以下两种更新：

### 1. Bootstrapped Sequential Updates

​	episodes 从记忆库中选取，更新从回合开始到结束，RNN的隐状态从回合开始至结束。

### 2. Bootstrapped Random Updates

​	episodes 从记忆库中选取，更新从随机点开始，RNN的初始状态为0在更新开始的时刻。

序列化的更新能够使得LSTM的隐状态从回合开始至结尾，但是其违反了DQN的随机采样的规定。

随机更新保持了随机采样的优势但是其中的LSTM的初始状态则每次需在更新时为零。使LSTM学习那些时间跨度长的函数效果减弱。

但是实验结果显示两种更新方式都能使得策略收敛并且其效果类似，于是选取随机更新的方式，更为简便，

## Evaluation standard Atari 2600

这里想办法引入部分可观测性，闪烁乒乓

实验证明了卷积神经网络可以取代在卷积神剧网络中叠加图像帧取得的效果，并且效果更好。

## Discussion&Conclusion

由于部分可观测性，真实任务中通常面对的是不完全的噪声状态数据，DRQN没有对输入图像帧进行堆叠，单凭一张图像帧就可以整合图像帧之间的状态信息，如物体的速度方向等。DRQN对于适应不同观测数据的完整性的鲁棒性很好，但是仅仅在乒乓球游戏中观察到了这种系统的提升表现。所以其实添加循环神经网络与堆叠图像帧的效果其实是类似的，

