# Strategic Attentive Writer for Learning Macro-Actions 2016

## Abstract

​	利用深度循环神经网络仅仅通过与环境的交互来端到端的学习构建隐式的计划，网络建立了一个内部的规划，并根据环境的观测更新改变，并能分裂这种内部表示成为一个序列根据他们的作用时间，综上产生了STRAW（STRategic Action Writer）。这些宏观动作可以使得探索结构化，同时可以使得计算量可观。并在文本预测上产生了较为不错的效果。

## Introduction

​	能够自动发现有用的宏观动作的代理可以有效的进行探索和学习，在强化学习和序列预测领域这种抽象的时间表示一直是一个问题，目前还没有成功的可伸缩的架构出现。

​	我们提出了一个新的深度循环网络架构，能够学习宏观操作，大部分RL算法每一次观测输出一个动作，但是STRAW算法输出一个多步的动作规划（方案），STRAW周期的根据观测更新方案并在重新规划决策点前提交更新的方案。重规划的决定以及经常出现的一些列动作（宏观动作）从奖励中学得，为了鼓励宏观动作的探索在卷积神经网络和规划模块之间建立了一个带噪声的通道，受到最近变分自动编码器的发展的启发，这种噪声在这个等级上能够给网络带来随机性，覆盖了多个时间步，从而创造出相应的效果。

​	Benefits:
​		1.促进了强化学习中结构化的探索，使得好的（有意义的）动作模式能够产生更长远的探索动作序列。
​		2.模型在提交动作规划时不需要处理观测值，从而可以使计算资源得以分配给更需要的部分。
​		3.学习宏观动作的架构可以拓展到其他领域。

## Related work

时间拓展动作和时间抽象学习是强化学习中一个现存的问题。
一种选择是带有时间终止条件的子策略，就是输出动作直到终止条件达到。但是需要伪奖励和人工设置的字母表的帮助，STRAW则不需要。

## The model

STRAW 分为两个模块：
	第一个模块负责将环境观测值翻译成动作计划，它代表了一个明确的未来的行动的随机规划，STRAW 通过提交计划并且应用这个规划在未来的几步中（不更新）来生成一个宏观的动作。
	第二个模块用于维护承诺规划，它能决定在哪一步网络终止宏观动作并且更新动作计划。
	动作规划是一个矩阵，矩阵的一个维度表示时间，另一个维度表示动作，矩阵的元素值表示在相应时刻采取某个动作的概率，承诺规划的元素值代表在相应时刻终止宏观动作的概率。
	更新两个规划的方法是注意力写作机制。这种方法能够使得网络聚焦于那些根据当前观测值能够产生想要的输出的计划上。

### The state of the network

A : action plan
c: commitment plan
$g_t$:截断或者使得macro action失效的时刻
$\rho$:  time-shift operator

### Attentive planning

​	支持宏观动作的一个重要的假设是一个观测值足以产生用来预测一系列未来动作的足够信息，注意力机制首先出现在图像领域，用于在空间上读写图像的像素信息，这里在计划的时间尺度上读写动作的概率，注意力机制的可微性使得其能够应用在反向传播上。

​	步长控制图像滤波器的尺度和分辨率，沿着时间尺度进行滤波。$\psi$表示注意力向量的参数（网格位置、步长、以及高斯滤波器的偏差）

action-plan update

![image-20200407110219421](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200407110219421.png)

commitment-plan update

Structured exploration with macro-actions

## Learning

loss function

![image-20200407195209889](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200407195209889.png)

## Experiments

Architecture  状态空间的特征表示在不同领域是不同的，图像是卷积神经网络，文本是循环神经网络。

Baselines: FF && LSTM

Optimization:使用A3C的架构（32）运行，optimizer为RMSprop。

### Text

------

LSTM   

### 2D mazes

------

用11x11的固定地图训练显示replanning的效果和关键点，15x15的地图中有困难，单独对目标点的移动作了处理，随机游走的步长逐渐增大，STRAW的时间抽象能力比LSTM好？

### Atari

------

在这个系列中选择那些需要进行规划和探索的游戏而不是单纯的靠反应力机制的游戏来检验，同时设置了一个反应力的游戏breakout来保证实验的完整性。

## Ablative analysis

action patch
code length
replanning modules

## Conclusion

能够端到端的非常明确的学习出宏观动作策略，并在强化学习AI和序贯决策领域开启了一个新的方向。对于那些决策性的领域启发性很大，但是同时要兼顾反应速度，运行的效率，在规划上花费的时间需要考虑怎么减少，如何平衡规划和反应的资源分配。







